import scrapy
from scrapy.spiders import Rule
from $project_name.settings import logger
from scrapy.linkextractors import LinkExtractor
from ayugespidertools.items import MysqlDataItem
from $project_name.common.DataEnum import TableEnum
from ayugespidertools.spiders import AyuCrawlSpider


class $classname(AyuCrawlSpider):
    name = "$name"
    allowed_domains = ["$domain"]
    start_urls = ["http://$domain/"]

    # 数据库表的枚举信息
    custom_table_enum = TableEnum
    # 初始化配置的类型
    settings_type = "debug"
    custom_settings = {
        # scrapy 日志等级配置
        "LOG_LEVEL": "DEBUG",
        "ITEM_PIPELINES": {
            # 激活此项则数据会存储至 Mysql
            "ayugespidertools.pipelines.AyuFtyMysqlPipeline": 300,
        },
        "DOWNLOADER_MIDDLEWARES": {
            # 随机请求头
            "ayugespidertools.middlewares.RandomRequestUaMiddleware": 400,
        },
    }

    rules = (
        # Rule(LinkExtractor(allow=r"Items/"), callback="parse_item", follow=True),
        Rule(LinkExtractor(restrict_xpaths='//div[@class="rank_d_b_name"]/a'), callback="parse_item"),
    )

    def parse_item(self, response):
        # 获取图书名称 - （获取的是详情页中的图书名称）
        book_name_list = response.xpath('//div[@class="book-name"]//text()').extract()
        book_name = "".join(book_name_list).strip()
        self.slog.debug(f"book_name: {book_name}")

        NovelInfoItem = MysqlDataItem(
            book_name=book_name,
            _table=TableEnum.article_list_table.value["value"],
        )
        yield NovelInfoItem
